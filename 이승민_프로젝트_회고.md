# 집값 예측 프로젝트

## 프로젝트 담당자
**이름:** 이승민

## 수행한 작업
### 1. 데이터 수집
주택 가격 예측에 필요한 다양한 환경 연관 변량 데이터를 수집하였습니다. 주요 수집 데이터는 다음과 같습니다:
- **서울시 지하철역 정보**
  - `서울시_역코드로_지하철역_정보_검색_호선기준_정렬.csv`
  - `서울시 역코드로 지하철역 정보 검색.csv`
- **공공하수처리시설 현황**
  - `한국환경공단_공공하수처리시설 현황_20211231.csv`
- **학교 기본 정보**
  - `서울시 고등학교 기본정보.csv`
  - `서울시 중학교 기본정보.csv`
- **지하철역 좌표 정보**
  - `서울교통공사_1_8호선 역사 좌표(위경도) 정보_20231031.csv`
- **코레일 주소 데이터**
  - `92. 코레일 주소데이터.csv`
- **하수도 및 부대시설 현황**
  - `하수도+및+부대시설+현황_20240627144048.csv`
- **서울시 공연장 인허가 정보**
  - `서울시 관광공연장업 인허가 정보.csv`
  - `서울시 공연장 인허가 정보.csv`
- **병원 인허가 정보**
  - `서울시_병원_인허가_정보_개업연월_위도경도_수정본.csv`

또한, 서울시 중학교와 고등학교 정보를 통해 각 학교의 위도와 경도 데이터를 수집하였습니다:
- `고등학교_변경.csv`
- `중학교_변경.csv`

이 데이터들은 주택 가격 예측에 있어 중요한 환경적 요인들을 반영하기 위해 수집되었습니다.

### 2. 데이터 스케일링
데이터 스케일링 과정은 `standard_scale.ipynb` 파일로 작성되었으며, Standard Scale 기법을 사용하여 데이터를 스케일링하였습니다. 아래는 해당 과정의 파이썬 코드와 설명입니다.

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# 문자열로 구성된 열 식별
string_columns = special_data.select_dtypes(include=['object']).columns

# 문자열 열 제거
special_data_cleaned = special_data.drop(columns=string_columns)

# '해제사유발생일'과 '세대135초과' 항목의 결측치를 0으로 처리
special_data_cleaned['해제사유발생일'] = special_data_cleaned['해제사유발생일'].fillna(0)
special_data_cleaned['세대135초과'] = special_data_cleaned['세대135초과'].fillna(0)

# '건축년도' 항목 제거
special_data_cleaned = special_data_cleaned.drop(columns=['건축년도'])

# 결측치가 있는 행 제거
special_data_cleaned = special_data_cleaned.dropna()

# '해제사유발생여부' 항목 생성
special_data_cleaned['해제사유발생여부'] = special_data_cleaned['해제사유발생일'].notna().astype(int)

# '해제사유발생일'을 연, 월, 일로 분리
special_data_cleaned['해제사유발생년'] = pd.to_datetime(special_data_cleaned['해제사유발생일']).dt.year
special_data_cleaned['해제사유발생월'] = pd.to_datetime(special_data_cleaned['해제사유발생일']).dt.month
special_data_cleaned['해제사유발생일'] = pd.to_datetime(special_data_cleaned['해제사유발생일']).dt.day

# '해제사유발생년', '해제사유발생월', '해제사유발생일' 항목 제거
special_data_cleaned = special_data_cleaned.drop(columns=['해제사유발생년', '해제사유발생월', '해제사유발생일'])

# 스케일링할 열 선택 (모든 열을 스케일링)
features_to_scale = special_data_cleaned.columns

# StandardScaler를 사용하여 데이터 스케일링
scaler = StandardScaler()
scaled_data = scaler.fit_transform(special_data_cleaned)
scaled_df = pd.DataFrame(scaled_data, columns=special_data_cleaned.columns)

# 각각의 피쳐를 그래프로 표현
fig, axs = plt.subplots(nrows=len(scaled_df.columns), ncols=1, figsize=(10, 5*len(scaled_df.columns)))

for i, column in enumerate(scaled_df.columns):
    axs[i].plot(scaled_df[column])
    axs[i].set_title(f'Scaled {column}')
    axs[i].set_xlabel('Index')
    axs[i].set_ylabel('Scaled Value')

plt.tight_layout()
plt.show()
```

위 과정에서는 먼저 문자열로 구성된 열을 식별하고 제거하였습니다. 그런 다음, 특정 열의 결측치를 0으로 처리하고 불필요한 열을 제거하였습니다. 이후 결측치가 있는 행을 제거하고, '해제사유발생여부' 항목을 새로 생성하였습니다. 마지막으로 모든 열을 StandardScaler를 사용하여 스케일링하였으며, 스케일링된 데이터를 그래프로 시각화하였습니다.

### 3. LGBM 모델링
LGBM 모델링 과정에서는 두 가지 주요 작업을 수행하였습니다. 이 과정은 `LGBM_na_train_final.ipynb` 파일에서 진행되었습니다.

#### 3-1. Feature Importance 분석
LGBM을 통해 각 변수의 중요도를 분석하였습니다. 그 과정은 다음과 같습니다:

1. `na_train_merge_최종.csv` 파일을 읽어 데이터프레임으로 변환하였습니다.

2. 문자열 데이터를 제외한 새로운 데이터프레임을 생성하였습니다.

3. '해제사유발생일' 항목의 결측치를 0으로, 값이 있으면 1로 바꾸고 '해제사유발생일' 항목을 제거하였습니다.

4. '연식' 항목을 생성하고, 결측치가 있는 행을 제거하였습니다.

5. 데이터프레임에서 feature와 target을 분리하고, 훈련 세트와 테스트 세트로 분할하였습니다.

6. LGBM 모델을 초기화하고 학습시킨 후, 특성 중요도를 분석하였습니다.

```python
import pandas as pd
import numpy as np
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

# 데이터 읽기
data = pd.read_csv('na_train_merge_최종.csv')

# 문자열 데이터를 제외한 새로운 데이터프레임 생성
na_train_final_numeric = data.select_dtypes(exclude=['object'])

# '해제사유발생일' 항목의 결측치를 0으로, 값이 있으면 1로 바꾸기
na_train_final_numeric['해제사유발생여부'] = na_train_final_numeric['해제사유발생일'].notnull().astype(int)

# '해제사유발생일' 항목 제거
na_train_final_numeric.drop(columns=['해제사유발생일'], inplace=True)

# '연식' 항목 생성 ('연도'에서 '건축년도'를 뺀 값)
na_train_final_numeric['연식'] = na_train_final_numeric['연도'] - na_train_final_numeric['건축년도']

# 결측치가 있는 행 제거
na_train_final_cleaned = na_train_final_numeric.dropna()

# 데이터 프레임에서 feature와 target 분리
X = na_train_final_cleaned.drop('target', axis=1)  # target 열 제외한 모든 열이 feature
y = na_train_final_cleaned['target']  # target 열

# 훈련 세트와 테스트 세트로 분할 (80:20 비율)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# LGBM 모델 초기화
model = LGBMRegressor(random_state=42)

# 모델 학습
model.fit(X_train, y_train)

# 특성 중요도 추출
importance = model.feature_importances_

# 특성 중요도를 데이터프레임으로 변환
feature_importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': importance
}).sort_values(by='Importance', ascending=False)

# 특성 중요도 시각화
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Feature Importance')
plt.show()

# 특성 중요도가 50 이상인 항목만 추출
important_features_df = feature_importance_df[feature_importance_df['Importance'] >= 50]

#### 3-2. 최적 모델 개발
LGBM 모델을 학습시키고, 하이퍼파라미터 튜닝을 통해 최적의 모델을 개발하였습니다. 주요 과정은 다음과 같습니다:

```python
# 특성 중요도가 50 이상인 항목 추출
important_features = feature_importance_df[feature_importance_df['Importance'] >= 50]['Feature']

# 원래 데이터 프레임에서 중요한 특성만 포함하는 부분 추출
important_features_df = na_train_final_cleaned[important_features]

# target 열 추가
important_features_df['target'] = na_train_final_cleaned['target']

# 데이터 프레임에서 feature와 target 분리
X_important = important_features_df.drop('target', axis=1)  # target 열 제외한 모든 열이 feature
y_important = important_features_df['target']  # target 열

# 훈련 세트와 테스트 세트로 분할 (80:20 비율)
X_train, X_test, y_train, y_test = train_test_split(X_important, y_important, test_size=0.2, random_state=42)

# LGBM 모델 초기화
model = LGBMRegressor(random_state=42)

# 모델 학습
model.fit(X_train, y_train)

# 테스트 세트에 대한 예측
y_pred = model.predict(X_test)

# 모델 성능 평가 (평균 제곱 오차 계산)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

# 예측 결과 시각화 (실제 값 vs 예측 값)
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.3)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=3)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted')
plt.show()

위 과정에서는 특성 중요도가 50 이상인 항목들을 추출하고, 이를 사용하여 새로운 데이터프레임을 생성하였습니다. 이후 훈련 세트와 테스트 세트로 데이터를 분할하고, LGBM 모델을 학습시켰습니다. 모델의 예측 성능을 평균 제곱 오차(MSE)로 평가한 결과, Mean Squared Error: 188257444.9579736이었습니다. 마지막으로, 실제 값과 예측 값을 시각화하여 비교하였습니다